### Intro

Abstractly, we can think of our computer as (ignoring io)
```
State : Type
computer : State -> State
```
In the case of a single process, we instead have something like (ignoring syscall io)
```
ProcessState : Type
data ProcessState = ProcessState Registers Memory
type Mem = ProcessState
runtime : Mem -> Mem
```

### Scope

We have an approximate idea of what our computation looks like when implemented in machine code.
This is absolutely not a hard rule: our executable could be an emulator for an exotic architecture
(redstone, lambda calculus, python) together with some hardcoded program. In those circumstances we
will realistically need to understand that architecture, then use (and likely also build our own)
tools specific for that architecture. The scope of our decompiler includes only "typical" machine
code for "typical" register machine architectures.

Beyond scalar/simd arithmetic, branches and load/store real architectures have
- Self-modifying code
- Memory-mapped-input-output (MMIO)
- Multithreading
- (non-syscall) Interrupts
- (maybe more I can't think of right now?)

We should probably consider all of these things out of scope, or at least unnecessary for now. I
think self-modifying code and MMIO could be handled with only minor modifications though.

### Constructing a dynamic CFG of BBs

At a given instant, we can identify some parts of `Mem` with Basic Blocks (BB). A BB is an interval
of machine code instructions not containing any branch/jump, followed by a branch/jump. In theory we
might find a BB within the jpg you just loaded into memory. In practice we only care about BBs that
are ever jumped to.

Whenever our main (and only) thread jumps to an address, we can lazily parse it as a BB if it is not
already. BBs have an extent, and we can invalidate the lazy cache after a write within the extent to
support self-modifying code (particularly important for dynamic loading). Note that BBs may overlap
if one is a suffix of the other: consider a NOP-sled.

We can SSA:ify (construct a single-static-assignment directed-acyclic-graph (DAG) from) any BB. A
set of BBs with edges between them denoting possible jumps is a control-flow-graph (CFG).

The typical decompiler approach (static CFG construction) is to assume non-self-modifying code, look
at memory, identify "function" start addresses, assume functions always return to their caller
through `ret` and convert local jumps to `while/for/dowhile`. This is essentially a process of
guessing the C code that compiled with `-O0` gives the correct machine code. This has some
advantages but I am more in a mood for talking about its disadvantages. These include:
- Cannot really be fuzzed/(formally reasoned about), because we have replaced defined ASM with
    undefined C.
- Might be excessively low level if we primarily care about higher level properties of our
    executable.

I propose we instead do dynamic CFG construction. We emulate the program one BB at a time, starting
at the entry point. Whenever a BB jumps to another BB, we parse it and add it and the edge between
the two to our CFG. A place in memory can contain a BB that we use, then be overwritten, then
contain another BB that we also use. These BBs can coexist in our dynamic CFG, so handling
self-modifying code is trivial.

### Using the CFG of BBs

Let's allow `Mem`, i.e. our registers, memory and potential other state (syscall handler?), to
contain not only words and bytes but also some kind of symbolic expressions (nodes in some DAG).
The instruction pointer must only contain concrete, non-symbolic values.

A BB might look something like
```
(BB123)
tmp1 = mem[..]
tmp2 = mem[..]
tmp3 = add tmp1 tmp2
mem[..] = tmp3
cnd = gt tmp1 tmp2
branch cnd BB125
jump BB124
```
Loads, Stores and Branches/Jumps can take
- Concrete and symbolic values
- Concrete addresses, or symbolic addresses which take on sufficiently few different
    values. We handle these by splitting our analysis and attempting all possible values.

ALU instructions can take arbitrary concrete and symbolic values. We cannot allow instructions
that symbolize too many entries of `Mem`.

Looking at single BBs, almost everything must be handled symbolically. Handling this is infeasible.
For example, a function exit BB can jump almost everywhere: it reads the return address and jumps to
it whatever its value!

But looking at the entire program, symbolic variables are far less necessary. If a function is has
only `N` call sites, we can end the BB with a `N`-way branch. If a function has a single call site,
the BB ends with an unconditional jump. This is the beauty of global analysis.

### Streaks

Our program starts as some initial state `Mem` were maybe a few places in memory contain symbolic
variables. We emulate it, one BB at a time. We add BBs to our CFG of BBs. Sometimes we
- Jump to an unparsed BB
  * Let's parse and construct it
- Jump to an already parsed BB
  * Let's generalize the BB to handle this parent BB, see LLVM `phi` instruction.
- Branch based on a symbolic variable.
  * Let's split our Streak and continue with independent `Mem`, while sharing the BB CFG.
- (I have probably missed some edge case)

This process continually builds a single BB CFG. We start with a single `Mem` which changes over
time, sometimes spliting due to symbolic variables affecting the global process state adversely.
This describes a rooted tree of `Mem`s, that we build iteratively and where we only need to
introspect (and hence store) the current leaves. This rooted tree is actually a DAG, since different
processes can converge (maybe a server zeroes its scratch space after finishing a request). I think
this convergence is uncommon in practice though (programs leak memory and increase, not decrease,
their entropy over their run time). I call a path of edges without branches or convergence a
**Streak**.

Let's assume we represent `Mem` using some persistent data structure. Then when this construction
stage ends (due to BBs using non-returning instructions like exit(), or a Streak ending in an
infinite loop of BBs) we have
- `MemDag`: A DAG of `Mem` containing all Streaks. We can introspect any register at any point along the
    process' execution.
- `BbCfg`: A BB CFG that shows the code that produced the `MemDag`. Branches/Jumps are simplified to
    where they are exactly as conditional as required by the actual program execution.

(Can you suggest better names for these data structures?)

For nontrivial programs we will need to add one more Streak/BB divergence case: the case where
following the edges out from the BB would make us exceed our arbitrarily chosen resource limit for
- number of edges out from a BB
- number of edges into a BB
- number of BBs in `MemDag`
- (Let's decide the details experimentally based on real-world programs)

There is no need (and also essentially impossible) to resolve the BBs beyond a smashed stack (doing
so by only sampling the space would be equivalent to automating ROPchain generation)

### Can we extend this to handle syscalls/nondeterminism/IO?

In reality, the initial `Mem` will not contain any symbolic variables. Symbolic variables are
introduced by syscalls/nondeterminism/IO, where we input a symbolic variable instead of running the
program on every possible input (which is infeasible).

There are probably multiple ways of incorporating IO. One weak approach (in the sense that I am sure it will work, but
it isn't maximally powerful) is to to the entire program analysis given a fixed
```
syscall_handler(last_event_id_in_this_streak, syscall_arg) -> (EventId, SyscallResult)
```
That is, if we want to change the result of a syscall we have discard all analysis of streaks
originating from this one and redo the analysis.

### Visualization? Symbolic solving?

Ideas:
- Decompilation view of `BbCfg` easy (transform BBs to branchless pseudo-C)
- Decompilation view of `BbCfg` medium (also identify sets of BB composing function bodies,
    recover for loops)
- Decompilation view of `BbCfg` advanced (guess variable data types)
- Guess function names by e.g. recognizing hash functions in the symbolic expressions within `Mem`.
- Execution trace view of `MemDag` (experiement! what data is useful for the user?)
- Identify lost control-flow-integrity
- Identify input that can be safely symbolized without blowing up the search space?
- SMT-solve specific streak ends. This entails solving a symbolic system of equations. Is that particular branch actually possible?
- Fuzztest specific streak ends. We can sample values hoping for a solution to our symbolic system
    of equations.
- All this while letting the user interactively specify `syscall_handler`?
- CRITICALLY: Failures should be understandable for the user! We must try to avoid a one-shot z3 job
    that chugs along for minutes without feedback.

Attempting a few of these seems realistically feasible.
