### Intro

Abstractly, we can think of our computer as (ignoring io)
```
State : Type
computer : State -> State
```
In the case of a single process, we instead have something like (ignoring syscall io)
```
ProcessState : Type
data ProcessState = ProcessState Registers Memory
type Mem = ProcessState
runtime : Mem -> Mem
```

### Scope

We have an approximate idea of what our computation looks like when implemented in machine code.
This is absolutely not a hard rule: our executable could be an emulator for an exotic architecture
(redstone, lambda calculus, python) together with some hardcoded program. In those circumstances we
will realistically need to understand that architecture, then use (and likely also build our own)
tools specific for that architecture. The scope of our decompiler includes only "typical" machine
code for "typical" register machine architectures.

Beyond scalar/simd arithmetic, branches and load/store real architectures have
- Self-modifying code
- Memory-mapped-input-output (MMIO)
- Multithreading
- (non-syscall) Interrupts
- (maybe more I can't think of right now?)
We should probably consider all of these things out of scope, or at least unnecessary for now. I
think self-modifying code and MMIO could be handled with only minor modifications though.

### Constructing a dynamic CFG of BBs

At a given instant, we can identify some parts of `Mem` with Basic Blocks (BB). A BB is an interval
of machine code instructions not containing any branch/jump, followed by a branch/jump. In theory we
might find a BB within the jpg you just loaded into memory. In practice we only care about BBs that
are ever jumped to.

Whenever our main (and only) thread jumps to an address, we can lazily parse it as a BB if it is not
already. BBs have an extent, and we can invalidate the lazy cache after a write within the extent to
support self-modifying code (particularly important for dynamic loading). Note that BBs may overlap
if one is a suffix of the other: consider a NOP-sled.

We can SSA:ify (construct a single-static-assignment directed-acyclic-graph (DAG) from) any BB. A
set of BBs with edges between them denoting possible jumps is a control-flow-graph (CFG).

The typical decompiler approach (static CFG construction) is to assume non-self-modifying code, look
at memory, identify "function" start addresses, assume functions always return to their caller
through `ret` and convert local jumps to `while/for/dowhile`. This is essentially a process of
guessing the C code that compiled with `-O0` gives the correct machine code. This has some
advantages but I am more in a mood for talking about its disadvantages. These include:
- Cannot really be fuzzed/(formally reasoned about), because we have replaced defined ASM with
    undefined C.
- Might be excessively low level if we primarily care about higher level properties of our
    executable.

I propose we instead do dynamic CFG construction. We emulate the program one BB at a time, starting
at the entry point. Whenever a BB jumps to another BB, we parse it and add it and the edge between
the two to our CFG. A place in memory can contain a BB that we use, then be overwritten, then
contain another BB that we also use. These BBs can coexist in our dynamic CFG, so handling
self-modifying code is trivial.

### Using the CFG of BBs

A BB might look something like
```
(BB123)
tmp1 = mem[..]
tmp2 = mem[..]
tmp3 = add tmp1 tmp2
mem[..] = tmp3
cnd = gt tmp1 tmp2
branch cnd BB125
jump BB124
```
I think all BB-exits can be represented as a some conditional branches followed by an unconditional
jump, if we allow jumping to addresses denoted by temporaries.

These jumps are somewhat restrictive, but most(all?) non-jump-instructions are not. Instructions
like `add` can be implemented for both concrete and symbolic variables.

TODO partially symbolic `mem`

TODO construct CFG of "streaks"="non-symbolic determined path of BBs". Streaks branch when symbolic
variable creates BB branch.

### Visualization? Partial fuzzing?

TODO

### Can we extend this to handle syscalls/nondeterminism/io?

There are probably multiple ways. One weak approach (in the sense that I am sure it will work, but
it isn't maximally powerful) is to to the entire program analysis given a fixed
```
syscall_server(last_event_id_in_this_timeline, syscall_arg) -> (EventId, SyscallResult)
```
That is, if we want to change the result of a syscall we have discard all analysis of timelines
following that result and redo the analysis.
